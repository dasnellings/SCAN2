#!/usr/bin/env python
# vim: set syntax=python

import math
import glob
import subprocess
import argparse
import os.path
import re
import yaml
import os
import pwd
import time
import uuid
import pysam       # just for reading BAM headers to find samples
import snakemake
import distutils.util # for strtobool
import csv
from sys import exit


# The default set must include ALL POSSIBLE parameters, even ones like BAM
# lists that will be empty at first.  This is because the default dict keys
# are used to determine which command line arguments are not to be stored
# in the configuration database.
scan2_param_defaults = {
    'analysis': 'joint_panel',
    'genome': 'CHM13v2.0',
    'is_started': False,
    'gatk': 'gatk3_joint',
    'parsimony_phasing': False,
    'target_fdr': 0.01,
    'rescue_target_fdr': 0.01,
    'snv_min_sc_alt': 2,
    'snv_min_sc_dp': 6,
    'snv_min_bulk_dp': 11, 
    'snv_max_bulk_alt': 0, 
    'snv_max_bulk_af': 0,
    'indel_min_sc_alt': 2,
    'indel_min_sc_dp': 10,
    'indel_min_bulk_dp': 11, 
    'indel_max_bulk_alt': 0, 
    'indel_max_bulk_af': 0,
    'min_base_quality_score': 20,
    'analyze_snvs': True,
    'analyze_indels': True,
    'analyze_mosaic_snvs': False,
    'callable_regions': True,
    'abmodel_use_fit': {},
    'abmodel_chunk_strategy': False,
    'abmodel_chunks': 10,
    'abmodel_samples_per_step': 20000,
    'abmodel_hsnp_tile_size': 100,
    'abmodel_hsnp_n_tiles': 250,
    'abmodel_refine_steps': 4,
    'abmodel_n_cores': 20,
    'digest_depth_n_cores': 20,
    'genotype_n_cores': 20,
    'integrate_table_n_cores': 20,
    'makepanel_n_cores': 20,
    'rescue_n_cores': 12,
    'resample_M': 20,
    'mimic_legacy': False,
    'bulk_sample': None,
    'gatk_chunks': None,
    'gatk_regions': [],
    'chrs': [],
    'phaser': 'shapeit',
    'shapeit_refpanel': None,  # a directory, so readable_file fails
    'cross_sample_panel': None, # must be readable IF specified, but pipeline should run without it too
    'scripts': '/home/boj924/SCAN2/scripts',  # also a directory
    'chr_prefix': '',          # be nice to remove this; requires snakefile changes
    'gvcf_map': {},             # maps sample name -> gatk path
    'permtool_config_map': {},
    'permtool_matrix_map': {},
    'permtool_n_permutations': 10000,
    'permtool_snv_generation_param': 100000,
    'permtool_indel_generation_param': 1/50,
    'permtool_callable_bed_n_cores': 20,
    'permtool_make_permutations_n_cores': 20,
    'permtool_combine_permutations_n_cores': 20
}

# Unlike params, all files listed here must be readable upon a call
# to validate() or run().
scan2_file_defaults = {
    'snakefile': '/home/boj924/SCAN2/Snakefile',
    'ref': None,
    'dbsnp': None,
    'gvcfs': {},         # miscellaneous BAMs; not genotyped as single cells
    'sc_gvcfs': {},      # BAMs for single cells, will be genotyped
    'bulk_gvcf': None,   # single bulk BAM for phasing, filtering germlines
    'scan2_objects': {},
    'add_muts': None,
    'makepanel_metadata': None,
    'permtool_muts': None,
    'permtool_bedtools_genome_file': None,
    'gatk_vcf': None,
    'eagle_refpanel': {},
    'eagle_genmap': None
}

scan2_defaults = { **scan2_param_defaults, **scan2_file_defaults }


class Analysis:
    """
    A SCAN2 analysis: metadata and actions.  Instantiation requires
    loading configuration data (and potentially Snakemake runtime
    data) from disk.
    """

    def __init__(self, analysis_dir):
        self.analysis_dir = analysis_dir
        self.cluster_log_dir = Analysis.make_logs_path(self.analysis_dir)
        self.config_path = Analysis.make_config_path(self.analysis_dir)
        self.load()

    @staticmethod
    def make_config_path(path):
        return os.path.abspath(os.path.join(path, 'scan.yaml'))

    @staticmethod
    def make_logs_path(path):
        return os.path.abspath(os.path.join(path, 'cluster-logs'))


    @staticmethod
    def create(args):
        """
        Create a new analysis object on disk and return an Analysis
        object pointing to it. Caution: will overwrite a previously
        existing SCAN2 analysis.
        Analysis is initialized with default parameters.
        """
        cpath = Analysis.make_config_path(args.analysis_dir)

        os.makedirs(args.analysis_dir, exist_ok=True)
        os.makedirs(Analysis.make_logs_path(args.analysis_dir),
            exist_ok=True)
        analysis_uuid = uuid.uuid4()   # A random ID
        ct = time.time()
        cd = time.strftime('%Y-%m-%d %H:%M %Z', time.localtime(ct))
        cfg = { 'creator': pwd.getpwuid(os.getuid()).pw_name,
                'create_time': ct,
                'create_date': cd,
                'analysis_uuid': str(analysis_uuid),
                **scan2_defaults
        }

        with open(cpath, 'w') as yfile:
            yaml.dump(cfg, yfile, default_flow_style=False)

        # Return the newly created object
        return Analysis(args.analysis_dir)


    def is_started(self):
        return self.cfg['is_started']


    def __str__(self):
        return "%s SCAN2 analysis type=%s (change with `config --analysis`), ID=%s" % \
            ('LOCKED' if self.is_started() else 'UNLOCKED',
             str(self.analysis),
             str(self.analysis_uuid))


    def show(self, verbose=False):
        """If 'verbose' is given, show all configuration values."""
        if verbose:
            return '\n'.join([ str(self) ] + \
                [ "%25s: %s" % (str(k), str(v)) \
                for k, v in self.cfg.items() ])

        else:
            return '\n'.join([ str(self),
                '%15s: %d single cell(s), %d bulk(s), %d other' % \
                    ('gvcfs', len(self.cfg['sc_gvcfs']),
                      self.cfg['bulk_gvcf'] is not None,
                      len(self.cfg['gvcfs'])),
                '%15s: %s' % ('Creator', self.cfg['creator']),
                '%15s: %s' % ('Create date', self.cfg['create_date']) ])


    def load(self):
        """Requires self.config_path to already be set."""
        with open(self.config_path, 'r') as yf:
            self.cfg = yaml.load(yf, Loader=yaml.FullLoader)
        self.analysis = self.cfg['analysis']
        self.analysis_uuid = self.cfg['analysis_uuid']


    def configure(self, args, verbose=False):
        """
        Merge new parameter values from 'args' into the previous config
        dictionary. New parameter specifications override old ones.  Since
        these values are not changeable once an analysis begins running,
        there is no issue with overwriting old values.

        'args' is the result of ArgumentParser.  Argument values will be
        None unless they were specified on the command line in *this*
        invocation of the scan2 script.
        """
        # First pass: get all new arguments and perform any special
        # handling. Second pass will tally updates and inform.
        new_cfg = {}
        for k, v in vars(args).items():
            if v is not None:
                for k2, v2 in self.handle_special(k, v).items():
                    new_cfg[k2] = v2
        
        # unusually special updates
        # 1. make the default value for chrs/gatk
        # regions when the user specifies the reference FASTA. But only
        # do this is the user did not also specify their own gatk regions
        # via --regions or --region-file in this call AND gatk_regions was
        # not set in a previous configure().
        if 'ref' in new_cfg and \
            'gatk_regions' not in new_cfg and self.cfg['gatk_regions'] == []:
            autosomes = get_autosomes_from_ref(new_cfg['ref'])
            new_cfg = { **new_cfg, **Analysis.handle_regions(autosomes) } # dict merge

        updates = 0
        print(new_cfg)
        for k, v in new_cfg.items():
            if k in self.cfg.keys() and v is not None and self.cfg[k] != v:
                if args.verbose:
                    print("Updating %s: %s -> %s" % (k, str(self.cfg[k]), str(v)))
                if k in scan2_file_defaults and type(v) == str:
                    # only top-level files are automatically converted,
                    # deeper files require handle_special().
                    v = readable_file(v)   # returns the absolute path
                self.cfg[k] = v
                updates = updates + 1

        # 2. Special update: sample -> gatk map
        if updates > 0:
            tups = []
            if self.cfg['bulk_gvcf'] is not None:
                tups.append((self.cfg['bulk_sample'], self.cfg['bulk_gvcf']))
            if len(self.cfg['sc_gvcfs']) > 0:
                tups = tups + [ s for s in self.cfg['sc_gvcfs'] ]
            if len(self.cfg['gvcfs']) > 0:
                tups = tups + [ s for s in self.cfg['gvcfs'] ]
            if len(tups) > 0:
                self.cfg['gvcf_map'] = tups
                print(self.cfg['gvcf_map'])

        if updates == 0:
            print("No changes to make to configuration. Stopping.")
        else:
            with open(self.config_path, 'w') as yf:
                yaml.dump(self.cfg, yf, default_flow_style=False)

    def handle_special(self, key, value):
        """
        Allow special handling of configuration parameters.  Any command
        line argument may expand into an arbitrarily large dict to be added
        to the configuration database.
        To skip special handling, simply return the original key and value.
        """
        if key in [ 'gvcf', 'sc_gvcf', 'bulk_gvcf', 'scan2_object' ]:
            if key in [ 'gvcf', 'sc_gvcf' ]:  # these provide lists
                return { key + 's' :  [readable_dir(gvcf) for gvcf in value ] }
            if key == 'bulk_gvcf':
                return { 'bulk_gvcf': readable_dir(value) }

        if key in [ 'regions', 'regions_file' ]:
            regions = regions_from_string(value) if \
                key == 'regions' else regions_from_file(value)
            return Analysis.handle_regions(regions)

        if key == 'gatk_vcf':
            # check that there is an associated VCF index file with the expected name
            readable_file(value + '.idx')
            return { 'gatk_vcf': readable_file(value) }

        # default: use the key=value pair as specified on command line
        return { key: value }



    @staticmethod
    def handle_regions(regions):
        """Extra handling for GATK genotyping intervals (regions)."""
        return { 'gatk_chunks': len(regions),
                 'gatk_regions': regions,
                 'chrs': chrs_from_regions(regions) }


    def check_scan2_objects(self):
        if len(self.cfg['scan2_objects']) == 0:
            error('no SCAN2 objects were specified (--scan2-object)')
        [ readable_file(r) for r in self.cfg['scan2_objects'].values() ]


    def check_gvcfs(self, error_on_missing=True):
        if error_on_missing and self.cfg['bulk_gvcf'] is None:
            error('no bulk gvcf was specified (--bulk-gvcf)')

        if error_on_missing and len(self.cfg['sc_gvcfs']) == 0:
            error('no single cell gvcfs were specified (--sc-gvcf)')

        all_gvcfs = []
        if self.cfg['bulk_gvcf'] is not None:
            print('adding bulk-gvcf')
            all_gvcfs += self.cfg['bulk_gvcf']  # not a list like others
        if len(self.cfg['sc_gvcfs']) > 0:
            print('adding sc-gvcf')
            all_gvcfs += self.cfg['sc_gvcfs']
        if self.cfg['gvcfs'] is not None:
            print('adding gvcf')
            all_gvcfs += self.cfg['gvcfs']

        [ readable_dir(b) for b in all_gvcfs ]
        # sometimes indexes convert .gvcf -> .bai rather than appending .gvcf -> .gvcf.bai
        #[ readable_file(b + '.bai') for b in all_gvcfs ]


    def check_gvcfs_makepanel(self):
        """
        Differs from above in that makepanel comes with a metadata file
        identifying single cell and bulk gvcfs. Make sure everything in
        the metadata file is present in the list of BAMs.
        """
        donors = []
        samples = []
        amps = []
        with open(self.cfg['makepanel_metadata'], 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                donors.append(row['donor'])
                samples.append(row['sample'])
                amps.append(row['amp'].lower())

        if len(set(donors)) == 1:
            warning("only a single donor was provided in --makepanel-metadata; indel results will be INVALID.")
            
        if amps.count('bulk') == 0:
            error("no BAMs were annotated as 'bulk' in --makepanel-metadata. At least one bulk BAM is required for panel building.")

        if amps.count('bulk') == len(amps):
            error("All BAMs were annotated as 'bulk' in --makepanel-metadata. At least one single cell BAM is required for panel building.")
            
    def validate(self):
        """
        Check that all necessary files exist and ensure the list of
        BAMs and single cell/bulk sample IDs make sense.
        """
        print("Checking BAMs..")
        self.check_gvcfs(error_on_missing=False)
        print("Checking panel metadata..")
        readable_file(self.cfg['makepanel_metadata'])
        self.check_gvcfs_makepanel()


def error(str):
    print("ERROR: " + str)
    exit(1)


def warning(str):
    print("WARNING: " + str)
    exit(1)


# Only checks that a file path can be read.
def readable_file(path):
    try:
        if path is None:
            raise IOError('file is not specified')
        with open(path, 'r') as f:
            return os.path.abspath(path)
    except IOError as err:
        error("file {0} could not be read:\n    {1}".format(path, err))

def readable_dir(path):
    if os.path.exists(path):
            return os.path.abspath(path)
    else:
        error("dir {0} is not valid:\n".format(path))


def check_refgenome(refgenome):
    """Check for FASTA index (.fai) and dictionary (.dict) files.
       Assumes refgenome has already been checked."""
    if refgenome is None:
        error("please provide a reference genome (--ref)")
    readable_file(refgenome + '.fai')
    readable_file(re.sub('.fasta$', '.dict', refgenome))


def get_autosomes_from_ref(refpath):
    """
    Get the correct names for chromosomes 1-22 from the FASTA header.
    ASSUMES the first 22 contigs in the header are the autosomes.
    """
    with open(refpath + '.fai', 'r') as f:
        return [ line.split('\t')[0] for line in f ][0:22]


#def check_dbsnp(dbsnp):
#    """Check for the VCF index file (.idx).  Assumes dbsnp has already
#       been checked."""
#    if dbsnp is None:
#        error("please provide a dbSNP VCF (--dbsnp)")
#    readable_file(dbsnp)
#    readable_file(dbsnp + '.idx')

def check_dbsnp(dbsnp):
    """Check for the VCF index file (.idx).  Assumes dbsnp has already
       been checked."""
    if dbsnp is None:
        error("please provide a dbSNP VCF (--dbsnp)")
    readable_file(dbsnp)
    readable_file(dbsnp + '.tbi')


def regions_from_string(regions):
    return args.regions.split(',')


def regions_from_file(regions_file):
    regions = []
    with open(regions_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            chrom, start, stop = line.strip().split('\t')[0:3]
            regions.append("{0}:{1}-{2}".format( chrom, int(start)+1, stop))
    return regions


def chrs_from_regions(regions):
    chrs = []
    # retains order
    rchrs = [ r.split(":")[0] for r in regions ]
    for c in rchrs:
        if chrs.count(c) == 0:
            chrs.append(c)

    return chrs



def do_init(args):
    try:
        # If an analysis exists, exception won't be thrown.
        a = Analysis(args.analysis_dir)
        print("ERROR: '%s' already contains a SCAN2 analysis." % \
            args.analysis_dir)
        print("Please delete the directory to create a new analysis.")
        print(a.show())
        exit(1)
    except FileNotFoundError:
        # Nothing on disk. Proceed.
        print("Creating new analysis with default parameters in '%s'.." % \
            args.analysis_dir)
        a = Analysis.create(args)
        print(a.show())
        print('Done.')
        print('Provide input files and set parameters via scan2 config.')
        print('Start the analysis with scan2 run.')


    
def do_config(args):
    a = Analysis(args.analysis_dir)

    if a.is_started():
        error("This analysis has already started and can no longer "
              "be configured.  If you wish to change runtime parameters, "
              "please see the 'run' subcommand.")

    a.configure(args)



def do_validate(args):
    a = Analysis(args.analysis_dir)
    a.validate()


# Runs a snakemake command provided by the user. Assumes 'args' contains
# runtime args.
def run_snakemake(snakemake_command, args, a):
    if args.memlimit:
        snakemake_command += [ "--resources",  "mem=" + str(args.memlimit) ]

    # handle --cluster or --drmaa and perform %logdir substitution
    if args.cluster is not None or args.drmaa is not None:
        if args.cluster is not None and args.drmaa is not None:
            error('only one of --cluster and --drmaa can be specified')
        if args.cluster is not None:
            snakemake_command += [ '--cluster',
                re.sub('%logdir', a.cluster_log_dir, args.cluster) ]
        else:
            snakemake_command += [ '--drmaa',
                re.sub('%logdir', a.cluster_log_dir, args.drmaa) ]

    # additional arbitrary snakemake args
    if args.snakemake_args:
        if args.snakemake_args[0] == ' ':
            args.snakemake_args = args.snakemake_args[1:]
        snakemake_command += args.snakemake_args.split(' ')

    snakemake.main(snakemake_command)


def do_joint_panel(args):
    a = Analysis(args.analysis_dir)

    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit)
    ]

    run_snakemake(snakemake_command, args, a)


def do_show(args):
    a = Analysis(args.analysis_dir)
    print(a.show(args.verbose))


# Would prefer not to have to parse this output, but learning to use the
# snakemake API directly is a project for a later time.
def parse_snakemake_job_report(process):
    jobdict = {}
    indata = False
    for line in process.stdout.decode().split('\n'):
        line = line.strip()
        if line == '':
            continue

        if line == 'Job counts:' or line == 'count\tjobs':
            indata = True
            continue

        if indata:
            try:
                count, rulename = line.split('\t')
                jobdict[rulename] = int(count)
            except ValueError:
                # Thrown on the last line, which does not have a rule name
                jobdict['__total'] = int(line)

    # when there's no jobs to run, snakemake prints nothing
    if len(jobdict) == 0:
        jobdict['__total'] = 0

    return jobdict


# 20 block progress bar
def progress_bar(x, total):
    blocks = math.floor(x/total*100/5)
    return '[' + '#'*blocks + ' '*(20-blocks) + '] %5d/%5d %0.1f%%' % (x, total, 100*x/total)


def do_progress(args):
    a = Analysis(args.analysis_dir)
    snakemake_command_status = [ "snakemake",
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--rerun-incomplete",
        "--quiet", "--dryrun" ]

    # Command above returns jobs that currently need to run.
    # Adding --forceall shows how many jobs there were to begin wtih.
    pstatus = subprocess.run(snakemake_command_status, capture_output=True)
    ptotal = subprocess.run(snakemake_command_status + [ '--forceall' ],
        capture_output=True)
    dstatus = parse_snakemake_job_report(pstatus)
    dtotal = parse_snakemake_job_report(ptotal)
    if args.verbose:
        print(a.show())
        print('')
        for k, v in dtotal.items():
            # __total is the total count, all is Snakemake's top-leel dummy rule
            if k != 'all':
                n_complete = v - dstatus.get(k, 0)
                k = 'Total progress' if k == '__total' else k
                print('%-40s %s' % (k, progress_bar(n_complete, v)))
    else:
        print('Total progress: %s' % \
            progress_bar(dtotal['__total'] - dstatus['__total'], dtotal['__total']))



##########################################################################
# Command line arguments and subcommand definitions.
##########################################################################

def add_runtime_args(parser):
    parser.add_argument('--joblimit', default=1, metavar='INT',
        help='Allow at most INT jobs to execute at any given time.  For '
            'multicore machines or clusters, this can greatly decrease '
            'runtime.')
    parser.add_argument('--memlimit', default=None, metavar='INT', type=int,
            help="Total available memory in MB.  If unspecified, memory is "
            "treated as unlimited.")
    parser.add_argument('--cluster', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --cluster parameter.  Do not use "
            "--snakemake-args to access --cluster.  Memory requirements "
            "for each job can be accessed via {resources.mem} and any "
            "instance of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--drmaa', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --drmaa parameter.  Do not use "
            "--snakemake-args to access --drmaa.  Memory requirements for "
            "each job can be accessed via {resources.mem} and any instance "
            "of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--snakemake-args', default='', type=str, metavar='STRING',
        help='Allows supplying arbitrary snakemake arguments in STRING. See snakemake --help for a list of parameters. Note that a leading space may be necessary, e.g., --snakemake-args " --dryrun".')

    return parser

if __name__ == "__main__":
    ap = argparse.ArgumentParser(prog='scan2',
        description='Somatic SNV genotyper for whole genome amplified '
                    'single cell sequencing experiments.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument('-d', '--analysis-dir', metavar='PATH', default='.',
        type=str,
        help='PATH containing the SCAN2 analysis. Required for all subcommands.')
    ap.add_argument('--snakefile', metavar='PATH', type=str,
        default='/home/boj924/SCAN2/Snakefile',
        help='Path to the main Snakefile.  Unlikely to be necessary for standard use.')
    subparsers = ap.add_subparsers(dest='subcommand')


    ##########################################################################
    # The 'init' subcommand
    #
    # 'init' only creates a new repository, nothing else. It is a separate
    # command to avoid accidental overwrites.
    ##########################################################################
    init_parser = subparsers.add_parser('init',
        help='Initialize a SCAN2 analysis directory')
    init_parser.set_defaults(executor=do_init)
    
    
    ##########################################################################
    # The 'config' subcommand
    #
    # Configuration variables need to be split into ones that can be
    # modified after analysis has run vs. ones that must be set first.
    #
    # Important: overwrite protection.
    ##########################################################################
    config_parser = subparsers.add_parser('config',
        help='Change configuration of a SCAN2 analysis. Note: config parameters '
            'cannot be changed after the analysis has begun.')
    config_parser.set_defaults(executor=do_config)
    
    config_parser.add_argument('--analysis', default='call_mutations',
        choices=[ 'call_mutations','joint_panel', 'makepanel', 'rescue', 'permtool' ],
        help='Analysis to run.\ncall_mutations: call somatic mutations.\nmakepanel: create a cross-sample panel suitable for --cross-sample-filter.\nrescue: perform signature-based rescue on SCAN2 objects generated by call_mutations.\npermtool: create control background distributions for somatic mutations by permutation. Mutations must be preprocesed to, e.g., remove duplicates.')
    config_parser.add_argument('--verbose', action='store_true', default=False,
        help='Print detailed list of all configuration changes.')
    config_parser.add_argument('--scripts', metavar='PATH',
        default='/home/boj924/SCAN2/scripts',
        help='Path to SCAN2 script files.  Usually points to an installed '
            'set of files.')
    config_parser.add_argument('--gatk', default='gatk3_joint',
        choices=[ 'gatk3_joint','gatk3_gvcf', 'gatk4_joint', 'gatk4_gvcf', 'sentieon_joint' ],
        help='GATK version and strategy for generating candidate somatic '
             'mutations and cross-sample support. Valid values are "gatk3_joint", '
             '"gatk4_joint", "gatk4_gvcf" and "sentieon_joint". Joint calling means that all BAMs '
             'are provided directly to HaplotypeCaller, whereas each BAM (sample) is '
             'initially analyzed separately in the GVCF strategy and combined '
             'later. The GVCF method can be lossy since all BAM data is not stored '
             'in the initial analysis; however, it does not require rerunning GATK '
             'across all samples in the multi-sample modes. The GVCF strategy is '
             'currently experimental and we do not recommend using it. When using'
             'sentieon_joint, the user must ensure the `sentieon` binary is in $PATH'
             'and that the environment variable SENTIEON_LICENSE points to a running'
             'sentieon license server.'
             'When --gatk-vcf VCF is used, the user must ensure that the chosen --gatk '
             'strategy matches the strategy used to generate VCF.')

    config_parser.add_argument('--gatk-vcf', type=str, metavar='VCF',
        help='SCAN2 runs GATK twice internally: once with a high read mapping quality '
             'threshold (mmq60) and once with a very low threshold (mmq1). Specifying '
             '--gatk-vcf skips the mmq60 run and uses VCF in its place, potentially '
             'saving a large amount of compute time. This option is especially useful '
             'when `scan2 makepanel` has been run to create a --cross-sample-panel: the '
             'GATK VCF stored in path/to/makepanel_output/gatk/hc_raw.mmq60.vcf is an '
             'appropriate input for --gatk-vcf.\n'
             'VERY IMPORTANT: VCF _must_ be generated by GATK in '
             '_exactly_ the way SCAN2 expects; this option is not meant to allow users '
             'to supply arbitrary inputs to SCAN2. DO THIS AT YOUR OWN RISK! There is '
             'no guarantee that SCAN2 will behave or even call mutations with high '
             'accuracy if the user ignores this advice and supplies a VCF generated by '
             'other means via --gatk-vcf. Furthermore, the --gatk strategy must '
             'match the strategy originally used to generate VCF.')
    config_parser.add_argument('--mimic-legacy', action='store_true', default=False,
        help="Mimic some behaviors of older SCAN2 versions. Not comprehensive.")
    config_parser.add_argument('--genotype-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to be used in the final genotyping step.")
    config_parser.add_argument('--integrate-table-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to be used when creating the integrated table.")
    config_parser.add_argument('--digest-depth-n-cores', default=20, type=int, metavar='N',
        help="Number of cores used to tabulate single cell and bulk sequencing "
             "depth.")
    config_parser.add_argument('--permtool-callable-bed-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to use when creating the BED of callable regions.")
    config_parser.add_argument('--permtool-make-permutations-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to use when generating shuffled mutation candidates.")
    config_parser.add_argument('--permtool-combine-permutations-n-cores', default=20, type=int, metavar='N',
        help="Number of cores to use when gathering per-sample permutations.")
    

    # required arguments
    req = config_parser.add_argument_group("External data")
    req.add_argument('--ref', type=readable_file, metavar='PATH',
        help='Path to reference genome FASTA file.  As required by GATK, '
             'two additional files must be present: an index (.fai) and a '
             'dictionary (.dict).')
    req.add_argument('--genome', type=str,
       choices=[ 'hs37d5', 'hg38', 'mm10', 'CHM13v2.0' ],
        help='Reference genome to use.  This is not checked against '
             'the provided reference genome or phasing panels, the user '
             'must ensure these match the supplied genome.  It is used '
             'when determining mutation signatures (e.g., to fetch '
             'trinucleotide contexts around candidate mutations).  Valid '
             'values must match those recognized by the R SCAN2 package '
             '(see the genome.string.to.bsgenome.object function).')
    req.add_argument('--dbsnp', type=readable_file, metavar='PATH',
        help='Path to a tribble-indexed (NOT tabix indexed) dbSNP VCF.')
    
    infiles = config_parser.add_argument_group('Input sequencing data',
        'At least two BAM files must be specified: one via --bulk-gvcf'
        ' and at least via one via --sc-gvcf.  Additional BAMs can be '
        ' specified by --gvcf: these will be used for GATK joint calling,'
        'but will affect the analysis in no other way.  This can be '
        'useful for adding additional bulks for followup comparisons.'
        'IMPORTANT: BAM files must be indexed (.bai) and must contain'
        ' only a single sample, identified by an @RG line with an SM '
        ' tag in the header.')
    infiles.add_argument('--bulk-gvcf', metavar='PATH',
        help='Matched bulk sample (not-single cell) for removing '
            'germline or clonal SNVs.  Only one sample may be '
            'designated as the bulk sample.')
    infiles.add_argument('--sc-gvcf', metavar='PATH', action='append',
        help='BAM corresponding to single cells that are to be '
            'analyzed.  May be specified multiple times.')
    infiles.add_argument('--gvcf',  action='append', metavar='PATH',
        help='Additional BAM files that will be included in GATK\'s'
            'joint genotyping, but will otherwise not affect the '
            'analysis.  May be specified several times.')
    
    
    
    gatk = config_parser.add_argument_group('Genotyping intervals',
        'These parameters allow the user to specify which portions of the genome should be genotyped.  By default, all autosomal regions will be analyzed.  Regions MUST be specified in the same order as the reference genome and should not overlap!!  The maximum target region is chrs 1-22 and X, due to the SHAPEIT reference panel.  Non-pseudoautosomal regions (PARs) on chrX may also be analyzed, but male samples may fail due to lack of hSNPs.  Initial GATK calling will be performed on each region in parallel if possible, which can greatly decrease runtime.')
    gatk.add_argument('--regions', metavar="STRING",
        help='A comma separated list of regions in GATK format: chr:start-stop, e.g. 22:30000001-31000000,22:31000001-32000000.  Cannot be specified in addition to --regions-file.')
    gatk.add_argument('--regions-file', metavar='PATH',
        help='A BED file containing regions to be analyzed.  Cannot be specified in addition to --regions.')
    
    
    joint_panel = config_parser.add_argument_group("Cross-sample filter creation (scan2 joint_panel)")
    joint_panel.add_argument('--makepanel-metadata', metavar='CSV', type=readable_file,
        help='CSV file containing metadata describing the samples in the panel. '
             'Must contain columns named: sample, donor and amp. The amp column '
             'is used to determine bulk vs. single cell status; amp=bulk (case '
             'sensitive) for bulks and any other value for single cells.')



    ##########################################################################
    # The 'validate' subcommand
    #
    # Tries to ensure the configuration is complete.
    validate_parser = subparsers.add_parser('validate',
        help='Validate the configuration parameters of a SCAN2 analysis.'
            '  [ATTENTION: not yet exhaustive.]')
    validate_parser.set_defaults(executor=do_validate)
    
    

    ##########################################################################
    # The 'joint_panel' subcommand
    #
    # Runs GATK HaplotypeCaller on a large set of BAMs, usually across cells
    # from multiple individuals. The goal is to generate a matrix of read
    # support for reference and alternate alleles at all sites with evidence
    # of non-reference bases.
    #
    # joint_panel runs an analysis, so runtime_args are appropriate.
    joint_panel_parser = subparsers.add_parser('joint_panel',
        help='Run GATK HaplotypeCaller jointly. Produces files necessary for '
             'indel calling (particularly, the cross-sample filter). IMPORTANT: '
             'If `joint_panel` output is not directly used as input to SCAN2 '
             'mutation calling, then the user should ensure that identical '
             '--gatk options are used between joint_panel and run.')
    joint_panel_parser.set_defaults(executor=do_joint_panel)
    joint_panel_parser = add_runtime_args(joint_panel_parser)
    joint_panel_parser.add_argument('--joint_panel-n-cores', type=int, default=20, metavar='N',
        help='Number of cores to use when digesting cross-sample panel GATK output.')
    
    
    # Get the args and run the relavent subcommand
    args = ap.parse_args()
    if args.subcommand is None:
        print("ERROR: a valid subcommand must be provided. See below.")
        ap.print_help()
        exit(1)
    
    args.analysis_dir = os.path.abspath(args.analysis_dir)
    args.executor(args)

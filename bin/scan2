#!/usr/bin/env python
# vim: set syntax=python

import math
import glob
import subprocess
import argparse
import os.path
import re
import yaml
import os
import pwd
import time
import uuid
import pysam       # just for reading BAM headers to find samples
import snakemake
import distutils.util # for strtobool
from sys import exit


# The default set must include ALL POSSIBLE parameters, even ones like BAM
# lists that will be empty at first.  This is because the default dict keys
# are used to determine which command line arguments are not to be stored
# in the configuration database.
scan2_param_defaults = {
    'genome': 'hs37d5',
    'analysis': 'call_mutations',
    'is_started': False,
    'gatk': 'gatk4_joint',
    'score_all_sites': True,
    'parsimony_phasing': False,
    'target_fdr': 0.01,
    'min_sc_alt': 2,
    'min_sc_dp': 6,
    'min_bulk_dp': 11, 
    'max_bulk_alt': 0, 
    'max_bulk_mosaic_vaf': 0.3,
    'min_base_quality_score': 20,
    'analyze_snvs': True,
    'analyze_indels': True,
    'analyze_mosaic_snvs': False,
    'callable_regions': True,
    'permute': 0,
    'abmodel_use_fit': {},
    'abmodel_chunk_strategy': False,
    'abmodel_chunks': 10,
    'abmodel_samples_per_step': 20000,
    'abmodel_hsnp_tile_size': 100,
    'abmodel_hsnp_n_tiles': 250,
    'abmodel_refine_steps': 4,
    'abmodel_n_cores': 1,
    'resample_M': 20,
    'bulk_sample': None,
    'gatk_chunks': None,
    'gatk_regions': [],
    'chrs': [],
    'phaser': 'shapeit',
    'shapeit_refpanel': None,  # a directory, so readable_file fails
    'scripts': '/opt/anaconda1anaconda2anaconda3/lib/scan2',  # also a directory
    'chr_prefix': '',          # be nice to remove this; requires snakefile changes
    'bam_map': {}              # maps sample name -> bam path
}

# Unlike params, all files listed here must be readable upon a call
# to validate() or run().
scan2_file_defaults = {
    'snakefile': '/opt/anaconda1anaconda2anaconda3/lib/scan2/Snakefile',
    'ref': None,
    'dbsnp': None,
    'bams': {},         # miscellaneous BAMs; not genotyped as single cells
    'sc_bams': {},      # BAMs for single cells, will be genotyped
    'bulk_bam': None,   # single bulk BAM for phasing, filtering germlines
    #'eagle_refpanel': {},
    #'eagle_genmap': None
}

scan2_defaults = { **scan2_param_defaults, **scan2_file_defaults }


class Analysis:
    """
    A SCAN2 analysis: metadata and actions.  Instantiation requires
    loading configuration data (and potentially Snakemake runtime
    data) from disk.
    """

    def __init__(self, analysis_dir):
        self.analysis_dir = analysis_dir
        self.cluster_log_dir = Analysis.make_logs_path(self.analysis_dir)
        self.config_path = Analysis.make_config_path(self.analysis_dir)
        self.load()

    @staticmethod
    def make_config_path(path):
        return os.path.abspath(os.path.join(path, 'scan.yaml'))

    @staticmethod
    def make_logs_path(path):
        return os.path.abspath(os.path.join(path, 'cluster-logs'))


    @staticmethod
    def create(args):
        """
        Create a new analysis object on disk and return an Analysis
        object pointing to it. Caution: will overwrite a previously
        existing SCAN2 analysis.
        Analysis is initialized with default parameters.
        """
        cpath = Analysis.make_config_path(args.analysis_dir)

        os.makedirs(args.analysis_dir, exist_ok=True)
        os.makedirs(Analysis.make_logs_path(args.analysis_dir),
            exist_ok=True)
        analysis_uuid = uuid.uuid4()   # A random ID
        ct = time.time()
        cd = time.strftime('%Y-%m-%d %H:%M %Z', time.localtime(ct))
        cfg = { 'creator': pwd.getpwuid(os.getuid()).pw_name,
                'create_time': ct,
                'create_date': cd,
                'analysis_uuid': str(analysis_uuid),
                **scan2_defaults
        }

        with open(cpath, 'w') as yfile:
            yaml.dump(cfg, yfile, default_flow_style=False)

        # Return the newly created object
        return Analysis(args.analysis_dir)


    def is_started(self):
        return self.cfg['is_started']


    def __str__(self):
        return "%s SCAN2 analysis ID=%s" % \
            ('LOCKED' if self.is_started() else 'UNLOCKED',
             str(self.analysis_uuid))


    def show(self, verbose=False):
        """If 'verbose' is given, show all configuration values."""
        if verbose:
            return '\n'.join([ str(self) ] + \
                [ "%25s: %s" % (str(k), str(v)) \
                for k, v in self.cfg.items() ])
        else:
            return '\n'.join([ str(self),
                '%15s: %d single cell(s), %d bulk(s), %d other' % \
                    ('BAMs', len(self.cfg['sc_bams']),
                      self.cfg['bulk_bam'] is not None,
                      len(self.cfg['bams'])),
                '%15s: %s' % ('Creator', self.cfg['creator']),
                '%15s: %s' % ('Create date', self.cfg['create_date']) ])


    def load(self):
        """Requires self.config_path to already be set."""
        with open(self.config_path, 'r') as yf:
            self.cfg = yaml.load(yf, Loader=yaml.FullLoader)
        self.analysis_uuid = self.cfg['analysis_uuid']


    def configure(self, args, verbose=False):
        """
        Merge new parameter values from 'args' into the previous config
        dictionary. New parameter specifications override old ones.  Since
        these values are not changeable once an analysis begins running,
        there is no issue with overwriting old values.

        'args' is the result of ArgumentParser.  Argument values will be
        None unless they were specified on the command line in *this*
        invocation of the scan2 script.
        """
        # First pass: get all new arguments and perform any special
        # handling. Second pass will tally updates and inform.
        new_cfg = {}
        for k, v in vars(args).items():
            if v is not None:
                for k2, v2 in self.handle_special(k, v).items():
                    new_cfg[k2] = v2
        
        # unusually special updates
        # 1. make the default value for chrs/gatk
        # regions when the user specifies the reference FASTA. But only
        # do this is the user did not also specify their own gatk regions
        # via --regions or --region-file in this call AND gatk_regions was
        # not set in a previous configure().
        if 'ref' in new_cfg and \
            'gatk_regions' not in new_cfg and self.cfg['gatk_regions'] == []:
            autosomes = get_autosomes_from_ref(new_cfg['ref'])
            new_cfg = { **new_cfg, **Analysis.handle_regions(autosomes) } # dict merge

        updates = 0
        for k, v in new_cfg.items():
            if k in self.cfg.keys() and v is not None and self.cfg[k] != v:
                if args.verbose:
                    print("Updating %s: %s -> %s" % (k, str(self.cfg[k]), str(v)))
                if k in scan2_file_defaults and type(v) == str:
                    # only top-level files are automatically converted,
                    # deeper files must require handle_special().
                    v = readable_file(v)   # returns the absolute path
                self.cfg[k] = v
                updates = updates + 1

        # 2. Special update: sample -> bam map
        if updates > 0:
            tups = []
            if self.cfg['bulk_bam'] is not None:
                tups.append((self.cfg['bulk_sample'], self.cfg['bulk_bam']))
            if len(self.cfg['sc_bams']) > 0:
                tups = tups + [ (s, b) for s, b in self.cfg['sc_bams'].items() ]
            if len(self.cfg['bams']) > 0:
                tups = tups + [ (s, b) for s, b in self.cfg['bams'].items() ]
            if len(tups) > 0:
                self.cfg['bam_map'] = dict(tups)
                print(self.cfg['bam_map'])

        if updates == 0:
            print("No changes to make to configuration. Stopping.")
        else:
            with open(self.config_path, 'w') as yf:
                yaml.dump(self.cfg, yf, default_flow_style=False)


    def handle_special(self, key, value):
        """
        Allow special handling of configuration parameters.  Any command
        line argument may expand into an arbitrarily large dict to be added
        to the configuration database.
        To skip special handling, simply return the original key and value.
        """
        if key in [ 'bam', 'sc_bam', 'bulk_bam' ]:
            if key in [ 'bam', 'sc_bam' ]:  # sc_bam and bam provide a lists
                return { key + 's' :
                    dict([ (sample_from_bam(bam), readable_file(bam)) for bam in value ]) }
            if key == 'bulk_bam':
                return { 'bulk_bam': readable_file(value),
                         'bulk_sample': sample_from_bam(value) }

        if key in [ 'regions', 'regions_file' ]:
            regions = regions_from_string(value) if \
                key == 'regions' else regions_from_file(value)
            return Analysis.handle_regions(regions)

        if key == 'abmodel_use_fit':
            # this is a list of tuples
            return { key : dict([ (sample, readable_file(file)) for sample, file in value ]) }

        if key == 'phaser':
            return { 'phaser': value.lower() }   # Just ensure lowercase

        #if key in [ 'eagle_refpanel', 'eagle_genmap' ]:
            #error("sorry, Eagle phasing is currently unsupported.")

        # default: use the key=value pair as specified on command line
        return { key: value }


    @staticmethod
    def handle_regions(regions):
        """Extra handling for GATK genotyping intervals (regions)."""
        return { 'gatk_chunks': len(regions),
                 'gatk_regions': regions,
                 'chrs': chrs_from_regions(regions) }


    def check_bams(self):
        if self.cfg['bulk_bam'] is None:
            error('no bulk BAM was specified (--bulk-bam)')

        if len(self.cfg['sc_bams']) == 0:
            error('no single cell BAMs were specified (--sc-bam)')

        [ readable_file(b) for b in self.cfg['sc_bams'].values() ]
        if len(self.cfg['bams']) > 0:  # miscellaneous BAMs aren't required
            [ readable_file(b) for b in self.cfg['bams'].values() ]
        readable_file(self.cfg['bulk_bam'])


    def check_phaser(self):
        # Determine the phaser and check relevant files.
        if self.cfg['phaser'] == 'shapeit':
            #readable_file(self.cfg['shapeit_refpanel'])
            check_shapeit(self.cfg['shapeit_refpanel'])
        elif self.cfg['phaser'] == 'eagle':
            if self.cfg['eagle_panel'] is None:
                print("ERROR: --eagle-panel must be specified when --phaser=eagle")
                exit(1)
            if self.cfg['eagle_genmap'] is None:
                print("ERROR: --eagle-genmap must be specified when --phaser=eagle")
                exit(1)
            args.eagle_panel = os.path.abspath(args.eagle_panel)
            args.eagle_genmap = readable_file(args.eagle_genmap)
            check_eagle(args.eagle_panel, chrs)
        else:
            error("--phaser must be either 'shapeit' or 'eagle'\n")


    def validate(self):
        """
        Check that all necessary files exist and ensure the list of
        BAMs and single cell/bulk sample IDs make sense.
        """
        # Additional checks: index files
        print("Checking reference genome..")
        check_refgenome(self.cfg['ref'])
        print("Checking dbSNP VCF..")
        check_dbsnp(self.cfg['dbsnp'])
    
        # Additional checks: indexes, at least one bulk and one single cell
        print("Checking BAMs..")
        self.check_bams()

        # Additional checks: very specific reference panel layout
        print("Checking phasing reference panel..")
        self.check_phaser()



def error(str):
    print("ERROR: " + str)
    exit(1)


# Only checks that a file path can be read.
def readable_file(path):
    try:
        if path is None:
            raise IOError('file is not specified')
        with open(path, 'r') as f:
            return os.path.abspath(path)
    except IOError as err:
        error("file {0} could not be read:\n    {1}".format(path, err))


def check_refgenome(refgenome):
    """Check for FASTA index (.fai) and dictionary (.dict) files.
       Assumes refgenome has already been checked."""
    if refgenome is None:
        error("please provide a reference genome (--ref)")
    readable_file(refgenome + '.fai')
    readable_file(re.sub('.fasta$', '.dict', refgenome))


def get_autosomes_from_ref(refpath):
    """
    Get the correct names for chromosomes 1-22 from the FASTA header.
    ASSUMES the first 22 contigs in the header are the autosomes.
    """
    with open(refpath + '.fai', 'r') as f:
        return [ line.split('\t')[0] for line in f ][0:22]


def check_dbsnp(dbsnp):
    """Check for the VCF index file (.idx).  Assumes dbsnp has already
       been checked."""
    if dbsnp is None:
        error("please provide a dbSNP VCF (--dbsnp)")
    readable_file(dbsnp)
    readable_file(dbsnp + '.idx')


def check_shapeit(shapeit_panel):
    """Check all required SHAPEIT haplotype panel files. Exit if any are missing."""
    if shapeit_panel is None:
        error("no SHAPEIT reference panel provided (--shapeit-refpanel)")
    if not os.path.exists(shapeit_panel):
        error("SHAPEIT panel path does not exist: " + shapeit_panel)
    if not os.path.isdir(shapeit_panel):
        error("SHAPEIT panel path is not a directory; " + shapeit_panel)
    for i in range(1, 23):
        fname = 'genetic_map_chr{0}_combined_b37.txt'.format(i)
        readable_file(os.path.join(shapeit_panel, fname))
        for suf in [ 'hap', 'legend' ]:
            fname = "1000GP_Phase3_chr{0}.{1}.gz".format(i, suf)
            readable_file(os.path.join(shapeit_panel, fname))

    # chrX is not consistently named
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.hap.gz"))
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.legend.gz"))
    readable_file(os.path.join(shapeit_panel, "genetic_map_chrX_nonPAR_combined_b37.txt"))


def eagle_panel_files(path, chrs):
    files = {}
    for s in chrs:
        f = glob.glob(os.path.join(path, 'ALL.{0}[\._]*.bcf'.format(s)))[0]
        files[s] = f
    return files


def check_eagle(eagle_panel, chrs):
    for f in eagle_panel_files(eagle_panel, chrs).values():
        readable_file(f)


def sample_from_bam(bampath):
    with pysam.AlignmentFile(bampath, 'r') as af:
        sample = af.header.to_dict()['RG'][0]['SM']
    print('Got sample name "%s" from BAM %s' % (sample, bampath))
    return sample




def regions_from_string(regions):
    return args.regions.split(',')


def regions_from_file(regions_file):
    regions = []
    with open(regions_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            chrom, start, stop = line.strip().split('\t')[0:3]
            regions.append("{0}:{1}-{2}".format( chrom, int(start)+1, stop))
    return regions


def chrs_from_regions(regions):
    chrs = []
    # retains order
    rchrs = [ r.split(":")[0] for r in regions ]
    for c in rchrs:
        if chrs.count(c) == 0:
            chrs.append(c)

    return chrs






def do_init(args):
    try:
        # If an analysis exists, exception won't be thrown.
        a = Analysis(args.analysis_dir)
        print("ERROR: '%s' already contains a SCAN2 analysis." % \
            args.analysis_dir)
        print("Please delete the directory to create a new analysis.")
        print(a.show())
        exit(1)
    except FileNotFoundError:
        # Nothing on disk. Proceed.
        print("Creating new analysis with default parameters in '%s'.." % \
            args.analysis_dir)
        a = Analysis.create(args)
        print(a.show())
        print('Done.')
        print('Provide input files and set parameters via scan2 config.')
        print('Start the analysis with scan2 run.')


    
def do_config(args):
    a = Analysis(args.analysis_dir)

    if a.is_started():
        error("This analysis has already started and can no longer "
              "be configured.  If you wish to change runtime parameters, "
              "please see the 'run' subcommand.")

    a.configure(args)



def do_validate(args):
    a = Analysis(args.analysis_dir)
    a.validate()



def do_run(args):
    a = Analysis(args.analysis_dir)

    # I would like to use the snakemake API directly (i.e., call snakemake()),
    # but this makes it difficult to allow the user to supply arbitrary
    # snakemake options.
    # Must be a vector of strings as would be expected in argv[1:].
    # N.B., argv[0] is the invoking executable and should not be specified.
    from snakemake import main
    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit),
    ]

    run_snakemake(snakemake_command, args, a)


# Runs a snakemake command provided by the user. Assumes 'args' contains
# runtime args.
def run_snakemake(snakemake_command, args, a):
    if args.memlimit:
        snakemake_command += [ "--resources",  "mem=" + str(args.memlimit) ]

    # handle --cluster or --drmaa and perform %logdir substitution
    if args.cluster is not None or args.drmaa is not None:
        if args.cluster is not None and args.drmaa is not None:
            error('only one of --cluster and --drmaa can be specified')
        if args.cluster is not None:
            snakemake_command += [ '--cluster',
                re.sub('%logdir', a.cluster_log_dir, args.cluster) ]
        else:
            snakemake_command += [ '--drmaa',
                re.sub('%logdir', a.cluster_log_dir, args.drmaa) ]

    # additional arbitrary snakemake args
    if args.snakemake_args:
        if args.snakemake_args[0] == ' ':
            args.snakemake_args = args.snakemake_args[1:]
        snakemake_command += args.snakemake_args.split(' ')

    snakemake.main(snakemake_command)


def do_makepanel(args):
    a = Analysis(args.analysis_dir)

    from snakemake import main
    snakemake_command = [
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--latency-wait", "30",
        "--rerun-incomplete",
        "--jobs", str(args.joblimit),
    ]

    run_snakemake(snakemake_command, args, a)


# Runs a snakemake command provided by the user. Assumes 'args' contains


def do_multi(args):
    print('SCAN2 multi: implementation underway')


def do_show(args):
    a = Analysis(args.analysis_dir)
    print(a.show(args.verbose))


# Would prefer not to have to parse this output, but learning to use the
# snakemake API directly is a project for a later time.
def parse_snakemake_job_report(process):
    jobdict = {}
    indata = False
    for line in process.stdout.decode().split('\n'):
        line = line.strip()
        if line == '':
            continue

        if line == 'Job counts:' or line == 'count\tjobs':
            indata = True
            continue

        if indata:
            try:
                count, rulename = line.split('\t')
                jobdict[rulename] = int(count)
            except ValueError:
                # Thrown on the last line, which does not have a rule name
                jobdict['__total'] = int(line)

    # when there's no jobs to run, snakemake prints nothing
    if len(jobdict) == 0:
        jobdict['__total'] = 0

    return jobdict


# 20 block progress bar
def progress_bar(x, total):
    blocks = math.floor(x/total*100/5)
    return '[' + '#'*blocks + ' '*(20-blocks) + '] %5d/%5d %0.1f%%' % (x, total, 100*x/total)


def do_progress(args):
    a = Analysis(args.analysis_dir)
    snakemake_command_status = [ "snakemake",
        "--snakefile",  args.snakefile,
        "--configfile", a.config_path,
        "--directory", a.analysis_dir,
        "--rerun-incomplete",
        "--quiet", "--dryrun" ]

    # Command above returns jobs that currently need to run.
    # Adding --forceall shows how many jobs there were to begin wtih.
    pstatus = subprocess.run(snakemake_command_status, capture_output=True)
    ptotal = subprocess.run(snakemake_command_status + [ '--forceall' ],
        capture_output=True)
    dstatus = parse_snakemake_job_report(pstatus)
    dtotal = parse_snakemake_job_report(ptotal)
    if args.verbose:
        print(a.show())
        print('')
        for k, v in dtotal.items():
            # __total is the total count, all is Snakemake's top-leel dummy rule
            if k != 'all':
                n_complete = v - dstatus.get(k, 0)
                k = 'Total progress' if k == '__total' else k
                print('%-40s %s' % (k, progress_bar(n_complete, v)))
    else:
        print('Total progress: %s' % \
            progress_bar(dtotal['__total'] - dstatus['__total'], dtotal['__total']))



##########################################################################
# Command line arguments and subcommand definitions.
##########################################################################

def add_runtime_args(parser):
    parser.add_argument('--joblimit', default=1, metavar='INT',
        help='Allow at most INT jobs to execute at any given time.  For '
            'multicore machines or clusters, this can greatly decrease '
            'runtime.')
    parser.add_argument('--memlimit', default=None, metavar='INT', type=int,
            help="Total available memory in MB.  If unspecified, memory is "
            "treated as unlimited.")
    parser.add_argument('--cluster', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --cluster parameter.  Do not use "
            "--snakemake-args to access --cluster.  Memory requirements "
            "for each job can be accessed via {resources.mem} and any "
            "instance of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--drmaa', default=None, type=str, metavar='ARGS',
        help="Pass ARGS to Snakemake's --drmaa parameter.  Do not use "
            "--snakemake-args to access --drmaa.  Memory requirements for "
            "each job can be accessed via {resources.mem} and any instance "
            "of '%%logdir' in ARGS will be replaced by "
            "--output-dir/cluster-logs.")
    parser.add_argument('--snakemake-args', default='', type=str, metavar='STRING',
        help='Allows supplying arbitrary snakemake arguments in STRING. See snakemake --help for a list of parameters. Note that a leading space may be necessary, e.g., --snakemake-args " --dryrun".')

    return parser

if __name__ == "__main__":
    ap = argparse.ArgumentParser(prog='scan2',
        description='Somatic SNV genotyper for whole genome amplified '
                    'single cell sequencing experiments.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument('-d', '--analysis-dir', metavar='PATH', default='.',
        type=str,
        help='PATH containing the SCAN2 analysis. Required for all subcommands.')
    ap.add_argument('--snakefile', metavar='PATH', type=str,
        default='/opt/anaconda1anaconda2anaconda3/lib/scan2/Snakefile',
        help='Path to the main Snakefile.  Unlikely to be necessary for standard use.')
    subparsers = ap.add_subparsers(dest='subcommand')


    ##########################################################################
    # The 'init' subcommand
    #
    # 'init' only creates a new repository, nothing else. It is a separate
    # command to avoid accidental overwrites.
    ##########################################################################
    init_parser = subparsers.add_parser('init',
        help='Initialize a SCAN2 analysis directory')
    init_parser.set_defaults(executor=do_init)
    
    
    ##########################################################################
    # The 'config' subcommand
    #
    # Configuration variables need to be split into ones that can be
    # modified after analysis has run vs. ones that must be set first.
    #
    # Important: overwrite protection.
    ##########################################################################
    config_parser = subparsers.add_parser('config',
        help='Change configuration of a SCAN2 analysis. Note: config parameters '
            'cannot be changed after the analysis has begun.')
    config_parser.set_defaults(executor=do_config)
    
    config_parser.add_argument('--analysis', default='call_mutations',
        help='Which analysis should be run. Currently, "call_mutations" and '
             '"make_panel" are valid values.')
    config_parser.add_argument('--verbose', action='store_true', default=False,
        help='Print detailed list of all configuration changes.')
    config_parser.add_argument('--scripts', metavar='PATH',
        default='/opt/anaconda1anaconda2anaconda3/lib/scan2',
        help='Path to SCAN2 script files.  Usually points to an installed '
            'set of files.')
    #config_parser.add_argument('--permute', type=int,
        #help='Number of permutation sets to create for somatic SNVs and indels.')
    config_parser.add_argument('--gatk', type=str,
        help='GATK version and strategy for generating candidate somatic '
             'mutations and cross-sample support. Valid values are "gatk3_joint", '
             '"gatk4_joint", "gatk4_gvcf" and "vcf". Joint calling means that all BAMs '
             'are provided directly to HaplotypeCaller, whereas each BAM (sample) is '
             'initially analyzed separately in the GVCF strategy and combined '
             'later. The GVCF method can be lossy since all BAM data is not stored '
             'in the initial analysis; however, it does not require rerunning GATK '
             'across all samples in the multi-sample modes. The joint strategy, '
             'which was used in the publication, can be extremely slow, especially '
             'when the number of cells is large. The "vcf" option allows the user '
             'to provide their own VCF (which must be generated by GATK) to avoid '
             'running GATK again. This option '
             'is specifically provided to allow the output from `scan2 makepanel` to '
             'be reused. The path to the user-specified VCF must be supplied via '
             '--gatk-vcf.')
    config_parser.add_argument('--gatk-vcf', type=str, metavar='PATH',
        help="VCF file for the --gatk vcf option. Ignored if --gatk is not 'vcf'.")
    

    # required arguments
    req = config_parser.add_argument_group("External data")
    req.add_argument('--ref', type=readable_file, metavar='PATH',
        help='Path to reference genome FASTA file.  As required by GATK, '
             'two additional files must be present: an index (.fai) and a '
             'dictionary (.dict).')
    req.add_argument('--genome', type=str, metavar='hs37d5|hg38|mm10',
        help='Reference genome to use.  This is not cross-checked against '
             'the provided reference genome or phasing panels.  It is used '
             'when determining mutation signatures (e.g., to fetch '
             'trinucleotide contexts around candidate mutations).  Valid '
             'values must match those recognized by the R SCAN2 package '
             '(see the genome.string.to.bsgenome.object function).')
    req.add_argument('--dbsnp', type=readable_file, metavar='PATH',
        help='Path to a tribble-indexed (NOT tabix indexed) dbSNP VCF.')
    
    infiles = config_parser.add_argument_group('Input sequencing data',
        'At least two BAM files must be specified: one via --bulk-bam'
        ' and at least via one via --sc-bam.  Additional BAMs can be '
        ' specified by --bam: these will be used for GATK joint calling,'
        'but will affect the analysis in no other way.  This can be '
        'useful for adding additional bulks for followup comparisons.'
        'IMPORTANT: BAM files must be indexed (.bai) and must contain'
        ' only a single sample, identified by an @RG line with an SM '
        ' tag in the header.')
    infiles.add_argument('--bulk-bam', metavar='PATH',
        help='Matched bulk sample (not-single cell) for removing '
            'germline or clonal SNVs.  Only one sample may be '
            'designated as the bulk sample.')
    infiles.add_argument('--sc-bam', metavar='PATH', action='append',
        help='BAM corresponding to single cells that are to be '
            'analyzed.  May be specified multiple times.')
    infiles.add_argument('--bam',  action='append', metavar='PATH',
        help='Additional BAM files that will be included in GATK\'s'
            'joint genotyping, but will otherwise not affect the '
            'analysis.  May be specified several times.')
    
    
    
    gatk = config_parser.add_argument_group('Genotyping intervals',
        'These parameters allow the user to specify which portions of the genome should be genotyped.  By default, all autosomal regions will be analyzed.  Regions MUST be specified in the same order as the reference genome and should not overlap!!  The maximum target region is chrs 1-22 and X, due to the SHAPEIT reference panel.  Non-pseudoautosomal regions (PARs) on chrX may also be analyzed, but male samples may fail due to lack of hSNPs.  Initial GATK calling will be performed on each region in parallel if possible, which can greatly decrease runtime.')
    gatk.add_argument('--regions', metavar="STRING",
        help='A comma separated list of regions in GATK format: chr:start-stop, e.g. 22:30000001-31000000,22:31000001-32000000.  Cannot be specified in addition to --regions-file.')
    gatk.add_argument('--regions-file', metavar='PATH',
        help='A BED file containing regions to be analyzed.  Cannot be specified in addition to --regions.')
    
    
    
    caller = config_parser.add_argument_group("Somatic SNV calling parameters. A minimum requirement of 2 reads supporting a putative mutation is probably good practice at most sequencing depths.  However, the minimum total depth for single cell and bulk may need to be altered.  The defaults of 6 and 11, respectively, were successful on single cells with >25x and bulk with >30x mean coverage.")
    caller.add_argument('--target-fdr', type=float, metavar='FLOAT',
        help='Desired false discovery rate (FDR).  This is not formal FDR control via, e.g., q-values.  In general, lower values will increase specificity at the cost of sensitivity.')
    caller.add_argument('--min-sc-alt', type=int, metavar='INT',
        help='Reject somatic SNVs with fewer than INT reads carrying the mutation in single cells.')
    caller.add_argument('--min-sc-dp', type=int, metavar='INT',
        help='Reject somatic SNVs covered by fewer than INT reads in single cells.')
    caller.add_argument('--min-bulk-dp', type=int,  metavar='INT',
        help='Reject somatic SNVs covered by fewer than INT reads in bulk.')
    caller.add_argument('--max-bulk-alt', type=int,  metavar='INT',
        help='Reject somatic SNVs supported by more than INT reads in bulk.')
    caller.add_argument('--max-bulk-mosaic-vaf', type=float,  metavar='FLOAT',
        help='Reject mosaic SNVs with VAF > FLOAT in bulk. E.g., --max-bulk-mosaic-vaf=0.3.')
    caller.add_argument('--min-base-quality-score', type=int,
        help="Passed to GATK HaplotypeCaller's --min-base-quality-score. "
             "Currently only used in GATK4 GVCF.")

    caller.add_argument('--no-somatic-snvs', action='store_true',
        help='Do not genotype somatic SNVs.')
    caller.add_argument('--somatic-indels', action='store_true',
        help='Enables detection of somatic indels. If this option is specified, a panel of normals must be specified.')
    caller.add_argument('--somatic-indel-pon', type=str,
        help='Indel panel of normals to use for indel filtering. Indel calling is NOT precise without a panel of normals.')
    caller.add_argument('--mosaic-snvs', action='store_true',
        help='Enable detection of mosaic SNVs.')
    caller.add_argument('--callable-regions', metavar='true/false/yes/no',
        type=lambda x: bool(distutils.util.strtobool(x)),
        help='Determine the fraction of callable genome based on '
             'read depth in single cells and matched bulk. This callable '
             'genome fraction is necessary for estimating the genome-wide '
             'mutation burden. This flag is enabled by default; to disable, '
             'callable region calculation, use --calable-regions false.')
    caller.add_argument('--score-all-sites', action='store_true',
        help='Perform somatic genotyping calculations at ALL non-reference '
             'sites reported by GATK--including single read sequencing '
             'artifacts, germline mutations, etc. Enabling this option '
             'increases runtime of the genotype scatter and CIGAR op '
             'counting steps by 10-100x.')
    
    
    phasing = config_parser.add_argument_group('Phasing options',
        'Choose SHAPEIT2 or Eagle for phasing')
    phasing.add_argument('--phaser', metavar='eagle|shapeit', type=str,
        help='SHAPEIT2 does not provide a reference panel aligned to hg38. If your data are aligned to this genome version, please use Eagle instead. [IMPORTANT! Eagle is currently unsupported.]')
    phasing.add_argument('--shapeit-refpanel', metavar='DIR',
        help='Path to the 1000 genomes project phase 3 SHAPEIT panel.  At this time, other phasing panels are not supported. This panel is required if phaser=shapeit.')
    phasing.add_argument('--parsimony-phasing', action='store_true',
        help='Choose the phase of each training hSNP i to minimize |VAF_(i-1) - VAF_i|.  This overrides population-based phasing and does not produce well-phased mutations.  Instead, it ensures that switching errors (caused by panel phasing) in regions with allelic imbalance do not inflate model variance or lead to "ping-pong" allele balance estimates.')
    phasing.add_argument('--eagle-panel', metavar='DIR',
        help='Path to the an Eagle compatible phasing panel. The reference panel is a set of BCF files, one per chromosome. File names must match the pattern ALL.chr${chromosome}*.bcf. This parameter is required if phaser=eagle.')
    phasing.add_argument('--eagle-genmap', metavar='DIR',
        help='Path to Eagle\'s provided genetic map (can be found under the "tables" directory in the Eagle tarball). This is required if phaser=eagle.')
    
    
    abmodel = config_parser.add_argument_group("AB model fitting",
        "These parameters control the exhaustive parameter search used to fit an AB correlation function for each chromosome.  This is by far the most time consuming step of the calling process.  None of these parameters are used for subsequent AB inference.")
    abmodel.add_argument('--abmodel-n-cores', type=int, default=1, metavar='INT',
        help='Multithread with INT cores when fitting the AB model parameters.  This is the preferred way of fitting the AB model and contrasts with the chunk strategy, described below.')
    abmodel.add_argument('--abmodel-samples-per-step', type=int, metavar='INT',
        help='Sample the AB model log-likelihood function INT times during each parameter space refinement step.')
    abmodel.add_argument('--abmodel-hsnp-tile-size', type=int, metavar='INT',
        help='Approximate the AB model likelihood function for each chromosome by breaking hSNPs into non-overlapping tiles of size INT.  Larger values significantly increase runtime, but may be necessary for organisms with high SNP density (e.g., >1 hSNP per kilobase on average).')
    abmodel.add_argument('--abmodel-refine-steps', type=int, metavar='INT',
        help='Refine the parameter space for random sampling INT times.  After the first log-likelihood sampling has completed, a new parameter space is defined so as to contain the 50 points with highest log likelihood.')
    abmodel.add_argument('--abmodel-use-fit', nargs=2, action='append', metavar=('SAMPLE_ID', 'FITS.RDA'),
        help='Do not fit the AB model for sample SAMPLE_ID; instead, use the fits provided in FITS.RDA. This option may be specified multiple times.')
    abmodel.add_argument('--abmodel-chunk-strategy', action='store_true', default=False,
        help='Legacy strategy for fitting AB model parameters.  In the chunked strategy, the job of sampling --abmodel-samples-per-chunk per refinement step is split into --abmodel-chunks single threaded jobs.  This was useful before multithreading support was added to AB parameter fitting and, especially, before reduced tile sampling was found to be effective.  It is unlikely that users will ever want to opt in to this strategy.')
    abmodel.add_argument('--abmodel-chunks', type=int, metavar='INT',
        help='Parallelize AB model fitting by splitting each AB model sampling step into INT jobs per chromosome. Only used by --abmodel-chunk-strategy, which is no longer the preferred way of parallelizing AB model fitting.')
    
    
    
    spikein = config_parser.add_argument_group("hSNP spike-ins",
        'hSNP spike ins provide a way to estimate sensitivity and the effects of various filters.  hSNP spikeins are also used to train models for excess indel and read clipping, so at least a few thousand spikeins are desired.')
    spikein.add_argument('--resample-M', type=int, metavar="INT",
        help='Parameter for rejection sampling. Larger M will produce fewer hSNP spikein samples for sensitivity and total mutation burden estimation. However, M must be large enough to enable proper rejection sampling.')
    
    

    ##########################################################################
    # The 'validate' subcommand
    #
    # Tries to ensure the configuration is complete.
    validate_parser = subparsers.add_parser('validate',
        help='Validate the configuration parameters of a SCAN2 analysis.'
            '  [ATTENTION: not yet exhaustive.]')
    validate_parser.set_defaults(executor=do_validate)
    
    
    ##########################################################################
    # The 'run' subcommand
    #
    # This actually runs an analysis
    run_parser = subparsers.add_parser('run',
        help='Run a fully configured SCAN2 analysis.  Always run the '
            '"validate" subcommand first.')
    run_parser.set_defaults(executor=do_run)
    run_parser = add_runtime_args(run_parser)
    

    ##########################################################################
    # The 'multi' subcommand
    #
    # Runs analyses that require multiple samples (for mutation signature SNV
    # calling) or individuals (for indel calling).
    multi_parser = subparsers.add_parser('multi',
        help='Run cross-sample (SNV mutation signatures) or cross-individual '
             '(indel) analyses. In principle, mutation signature SNV analysis '
             'could be successful on a single cell if the cell has a very high '
             'mutation rate (e.g., several thousand SNV calls using a stringent '
             'FDR=1%% single sample mode). Similarly, indels may also be called '
             'using cells from a single individual by filtering indels that '
             'appear in more than one single cell. Neither of these analyses '
             'are currently supported, though we hope to provide an option '
             'soon.\n'
             'Inputs to "multi" runs are completed single sample SCAN2 runs.')
    multi_parser.set_defaults(executor=do_multi)
    multi_parser = add_runtime_args(multi_parser)

    
    ##########################################################################
    # The 'show' subcommand
    #
    # Show all configuration information. Maybe allow for serialization?
    # Could just dump the YAML.
    show_parser = subparsers.add_parser('show',
        help='Show the configuration of a SCAN2 analysis.')
    show_parser.set_defaults(executor=do_show)
    show_parser.add_argument('--verbose', action='store_true', default=False,
        help='Show all configuration variables and settings.')
    
    
    ##########################################################################
    # The 'progress' subcommand
    #
    # Print some information about the progress of the run:
    #   #Jobs complete / Total #Jobs
    #   #Jobs with errors
    progress_parser = subparsers.add_parser('progress',
        help='Prints statistics regarding the progress of a SCAN2 run.')
    progress_parser.set_defaults(executor=do_progress)
    progress_parser.add_argument('-v', '--verbose', action='store_true', default=False,
        help='Show progress for individual rules.')


    ##########################################################################
    # The 'makepanel' subcommand
    #
    # Runs GATK HaplotypeCaller on a large set of BAMs, usually across cells
    # from multiple individuals. The goal is to generate a matrix of read
    # support for reference and alternate alleles at all sites with evidence
    # of non-reference bases.
    #
    # makepanel runs an analysis, so runtime_args are appropriate.
    makepanel_parser = subparsers.add_parser('makepanel',
        help='Run GATK HaplotypeCaller jointly. Produces files necessary for '
             'indel calling (particularly, the cross-sample filter). IMPORTANT: '
             'If `makepanel` output is not directly used as input to SCAN2 '
             'mutation calling, then the user should ensure that identical '
             '--gatk options are used between makepanel and run.')
    makepanel_parser.set_defaults(executor=do_makepanel)
    makepanel_parser = add_runtime_args(makepanel_parser)
    
    
    # Get the args and run the relavent subcommand
    args = ap.parse_args()
    if args.subcommand is None:
        print("ERROR: a valid subcommand must be provided. See below.")
        ap.print_help()
        exit(1)
    
    args.analysis_dir = os.path.abspath(args.analysis_dir)
    args.executor(args)

#!/usr/bin/env python
# vim: set syntax=python
# This is a wrapper script for executing Snakemake on the SCAN-SNV Snakefile.

import glob
import subprocess
import argparse
import os.path
import re
import yaml
import os
import pwd
import time
import uuid
import pysam       # just for reading BAM headers to find samples
from sys import exit

scan2_defaults = {
    'is_started': False,
    'target_fdr': 0.1,
    'min_sc_alt': 2,
    'min_sc_dp': 6,
    'min_bulk_dp': 11, 
    'analyze_snvs': True,
    'analyze_indels': False,
    'analyze_mosaic_snvs': False,
    'callable_regions': True,
    'permute': True,
    'phaser': 'shapeit',
    'abmodel_chunks': 4,
    'abmodel_samples_per_chunk': 5000,
    'abmodel_hsnp_chunk_size': 100,
    'abmodel_steps': 4,
    'resample_M': 20,
    'scripts': '/opt/anaconda1anaconda2anaconda3/lib/scansnv',
    'snakefile': '/opt/anaconda1anaconda2anaconda3/lib/scansnv/Snakefile'
}


class Analysis:
    """
    A SCAN2 analysis: metadata and actions.  Instantiation requires
    loading configuration data (and potentially Snakemake runtime
    data) from disk.
    """

    def __init__(self, analysis_dir):
        self.analysis_dir = analysis_dir
        self.config_path = Analysis.make_config_path(self.analysis_dir)
        self.load()

    @staticmethod
    def make_config_path(path):
        return os.path.join(path, 'scan.yaml')

    @staticmethod
    def create(args):
        """
        Create a new analysis object on disk and return an Analysis
        object pointing to it. Caution: will overwrite a previously
        existing SCAN2 analysis.
        Analysis is initialized with default parameters.
        """
        cpath = Analysis.make_config_path(args.analysis_dir)

        os.makedirs(args.analysis_dir, exist_ok=True)
        analysis_uuid = uuid.uuid4()   # A random ID
        ct = time.time()
        cd = time.strftime('%Y-%m-%d %H:%M %Z', time.localtime(ct))
        cfg = { 'creator': pwd.getpwuid(os.getuid()).pw_name,
                'create_time': ct,
                'create_date': cd,
                'analysis_uuid': str(analysis_uuid),
                **scan2_defaults
        }

        with open(cpath, 'w') as yfile:
            yaml.dump(cfg, yfile, default_flow_style=False)

        # Return the newly created object
        return Analysis(args.analysis_dir)

    def is_started(self):
        return self.cfg['is_started']

    def __str__(self):
        return "SCAN2 analysis ID=" + str(self.analysis_uuid)

    def show(self):
        return "\tCreator:     %s\n\tCreate date: %s" % \
            (self.cfg['creator'], self.cfg['create_date'])

    def load(self):
        """Requires self.config_path to already be set."""
        with open(self.config_path, 'r') as yf:
            self.cfg = yaml.load(yf)
        self.analysis_uuid = self.cfg['analysis_uuid']

    def configure(self, args):
        """
        Merge new parameter values from 'args' into the previous config
        dictionary. New parameter specifications override old ones.  Since
        these values are not changeable once an analysis begins running,
        there is no issue with overwriting old values.
        """
        updates = 0
        for k, v in vars(args).items():
            if k in self.cfg.keys() and v is not None and self.cfg[k] != v:
                print("Updating %s: %s -> %s" % (k, str(self.cfg[k]), str(v)))
                self.cfg[k] = v
                updates = updates + 1

        if updates == 0:
            print("No changes to make to configuration. Stopping.")
        else:
            with open(self.config_path, 'w') as yf:
                yaml.dump(self.cfg, yf, default_flow_style=False)



def error(str):
    print("ERROR: " + str)
    exit(1)



# Only checks that a file path can be read.
def readable_file(path):
    try:
        with open(path, 'r') as f:
            return os.path.abspath(path)
    except IOError as err:
        error("file {0} could not be read:\n    {1}".format(path, err))



def check_refgenome(refgenome):
    """Check for FASTA index (.fai) and dictionary (.dict) files.
       Assumes refgenome has already been checked."""
    readable_file(refgenome + '.fai')
    readable_file(re.sub('.fasta$', '.dict', refgenome))



def check_dbsnp(dbsnp):
    """Check for the VCF index file (.idx).  Assumes dbsnp has already
       been checked."""
    readable_file(dbsnp + '.idx')



def check_shapeit(shapeit_panel):
    """Check all required SHAPEIT haplotype panel files. Exit if any are missing."""
    if not os.path.exists(shapeit_panel):
        print("SHAPEIT panel path does not exist: " + shapeit_panel)
    if not os.path.isdir(shapeit_panel):
        print("SHAPEIT panel path is not a directory; " + shapeit_panel)
    for i in range(1, 23):
        fname = 'genetic_map_chr{0}_combined_b37.txt'.format(i)
        readable_file(os.path.join(shapeit_panel, fname))
        for suf in [ 'hap', 'legend' ]:
            fname = "1000GP_Phase3_chr{0}.{1}.gz".format(i, suf)
            readable_file(os.path.join(shapeit_panel, fname))

    # chrX is not consistently named
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.hap.gz"))
    readable_file(os.path.join(shapeit_panel, "1000GP_Phase3_chrX_NONPAR.legend.gz"))
    readable_file(os.path.join(shapeit_panel, "genetic_map_chrX_nonPAR_combined_b37.txt"))


def eagle_panel_files(path, chrs):
    files = {}
    for s in chrs:
        f = glob.glob(os.path.join(path, 'ALL.{0}[\._]*.bcf'.format(s)))[0]
        files[s] = f
    return files


def check_eagle(eagle_panel, chrs):
    for f in eagle_panel_files(eagle_panel, chrs).values():
        readable_file(f)


def sample_from_bam(bampath):
    with open(bampath, 'r') as f:
        return f.header.to_dict()['RG'][0]['SM']


def check_bams(bamlist):
    samples = []
    for bam in bamlist:
        samples.append(sample_from_bam(bam))
        bams.append(readable_file(bam))

    return (samples, bams)



def check_regions(args):
    """Check the user specified regions or region file."""
    regions = []
    if args.regions is None and args.regions_file is None:
        regions = [ str(x) for x in range(1, 23) ]

    if not args.regions is None and not args.regions_file is None:
        error("only one of --regions or --regions-file can be supplied")

    if not args.regions is None:
        regions = args.regions.split(',')

    if not args.regions_file is None:
        with open(args.regions_file) as f:
            for line in f:
                if line.startswith("#"):
                    continue

                chrom, start, stop = line.strip().split('\t')[0:3]
                regions.append("{0}:{1}-{2}".format( chrom, int(start)+1, stop))

    chrs = []
    # retains order
    rchrs = [ r.split(":")[0] for r in regions ]
    for c in rchrs:
        if chrs.count(c) == 0:
            chrs.append(c)

    return (chrs, regions)



def do_init(args):
    try:
        # If an analysis exists, exception won't be thrown.
        a = Analysis(args.analysis_dir)
        print("ERROR: '%s' already contains a SCAN2 analysis." % \
            args.analysis_dir)
        print("Please delete the directory to create a new analysis.")
        print(a)
        print(a.show())
        exit(1)
    except FileNotFoundError:
        # Nothing on disk. Proceed.
        print("Creating new analysis with default parameters in '%s'.." % \
            args.analysis_dir)
        a = Analysis.create(args)
        print(a)
        print(a.show())
        print('Done.')
        print('Provide input files and set parameters via scan2 config.')
        print('Start the analysis with scan2 run.')


    
def do_config(args):
    a = Analysis(args.analysis_dir)

    if a.is_started():
        error("This analysis has already started and can no longer "
              "be configured.  If you wish to change runtime parameters, "
              "please see the 'run' subcommand.")

    a.configure(args)



def do_validate(args):
    """
    Check that certain necessary files exist and ensure the list of
    BAMs and single cell/bulk sample IDs make sense.
    """
    a = Analysis(args.analysis_path)

    # Some input files must be paired with auxiliary files like indexes. Ensure
    # these files exist before starting the pipeline to reduce possible headaches.
    check_refgenome(a.cfg['ref'])

    check_dbsnp(a.cfg['dbsnp'])

    if a.cfg['somatic_indels']:
        readable_file(a.cfg['somatic_indel_pon'])


    print(a.cfg['bam'])
    print(a.cfg['sc_bam'])
    print(a.cfg['bulk_bam'])
    exit(1)
    (samples, bams) = check_bams(args.bam)
    for sc in args.sc_sample:
        if not sc in samples:
            error("single cell sample {0} is not associated with a BAM file.".format(sc))
    if not args.bulk_sample in samples:
        error("bulk sample {0} is not associated with a BAM file.".format(args.bulk_sample))

    (chrs, regions) = check_regions(args)

    # Determine the phaser and check relevant files.
    args.phaser = args.phaser.lower()
    if args.phaser == 'shapeit':
        args.shapeit_panel = os.path.abspath(args.shapeit_panel)
        check_shapeit(args.shapeit_panel)
    elif args.phaser == 'eagle':
        if args.eagle_panel is None:
            print("ERROR: --eagle-panel must be specified when --phaser=eagle")
            exit(1)
        if args.eagle_genmap is None:
            print("ERROR: --eagle-genmap must be specified when --phaser=eagle")
            exit(1)
        args.eagle_panel = os.path.abspath(args.eagle_panel)
        args.eagle_genmap = readable_file(args.eagle_genmap)
        check_eagle(args.eagle_panel, chrs)
    else:
        print("ERROR: --phaser must be either 'shapeit' or 'eagle'\n")
        exit(1)



def do_run(args):
    snakemake_command = [
        "snakemake",
        "--snakefile",  args.snakefile,
        "--configfile", configfile,
        "--jobs", str(args.joblimit),
        "--latency-wait", "300",
        ]
    
    if args.memlimit:
        snakemake_command += [ "--resources",  "mem=" + str(args.memlimit) ]
    
    if args.resume:
        snakemake_command.append("--rerun-incomplete")
    
    if args.cluster is not None or args.drmaa is not None:
        if args.cluster is not None and args.drmaa is not None:
            error('only one of --cluster and --drmaa can be specified at once')
    
        #snakemake_command += [ "--cluster-config", args.clusterfile ]
        logdir = os.path.join(os.path.abspath(args.output_dir), 'cluster-logs')
    
        if args.cluster is not None:
            if '%logdir' in args.cluster:
                os.makedirs(logdir, exist_ok=True)
            snakemake_command += [ '--cluster',
                re.sub('%logdir', logdir, args.cluster) ]
        else:
            if '%logdir' in args.drmaa:
                os.makedirs(logdir, exist_ok=True)
            snakemake_command += [ '--drmaa',
                re.sub('%logdir', logdir, args.drmaa) ]
    
    
    if args.snakemake_args:
        if args.snakemake_args[0] == ' ':
            args.snakemake_args = args.snakemake_args[1:]
        snakemake_command += args.snakemake_args.split(' ')
    
    
    # So that we can wait for Snakemake to exit even when ctrl+C is given
    try:
        print(' '.join(snakemake_command))
        sp = subprocess.Popen(snakemake_command, cwd=args.output_dir)
        sp.wait()
    except KeyboardInterrupt:
        print("Waiting for Snakemake to respond to SIGINT...")
        sp.wait()



def do_show(args):
    a = Analysis(args.analysis_dir)
    print(a)
    print(a.show())



def do_progress(args):
    a = Analysis(args.analysis_dir)
    print(a)
    print(a.show())
    print("Progress: [ # / # ]")



##########################################################################
# Command line arguments and subcommand definitions.
##########################################################################

ap = argparse.ArgumentParser(prog='scan2',
    description='Somatic SNV genotyper for whole genome amplified '
                'single cell sequencing experiments.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
ap.add_argument('--analysis-dir', metavar='PATH', default='.', type=str,
    help='PATH containing the SCAN2 analysis. Required for all subcommands.')
subparsers = ap.add_subparsers(dest='subcommand')


##########################################################################
# The 'init' subcommand
#
# 'init' only creates a new repository, nothing else. It is a separate
# command to avoid accidental overwrites.
##########################################################################
init_parser = subparsers.add_parser('init',
    help='Initialize a SCAN2 analysis directory')
init_parser.set_defaults(executor=do_init)


##########################################################################
# The 'config' subcommand
#
# Configuration variables need to be split into ones that can be
# modified after analysis has run vs. ones that must be set first.
#
# Important: overwrite protection.
##########################################################################
config_parser = subparsers.add_parser('config',
    help='Change configuration of a SCAN2 analysis. Note: config parameters '
         'cannot be changed after the analysis has begun.')
config_parser.set_defaults(executor=do_config)

# required arguments
req = config_parser.add_argument_group("External data")
req.add_argument('--ref', type=readable_file, metavar='PATH',
    help='Path to reference genome FASTA file.  As required by GATK, two additional files must be present: an index (.fai) and a dictionary (.dict).')
req.add_argument('--dbsnp', type=readable_file, metavar='PATH',
    help='Path to a tribble-indexed (NOT tabix indexed) dbSNP VCF.')

infiles = config_parser.add_argument_group('Input sequencing data',
    'At least two BAM files must be specified: one via --bulk-bam'
    ' and at least via one via --sc-bam.  Additional BAMs can be '
    ' specified by --bam: these will be used for GATK joint calling,'
    'but will affect the analysis in no other way.  This can be '
    'useful for adding additional bulks for followup comparisons.'
    'IMPORTANT: BAM files must be indexed (.bai) and must contain'
    ' only a single sample, identified by an @RG line with an SM '
    ' tag in the header.')
infiles.add_argument('--bulk-bam', metavar='PATH',
    help='Matched bulk sample (not-single cell) for removing '
         'germline or clonal SNVs.  Only one sample may be '
         'designated as the bulk sample.')
infiles.add_argument('--sc-bam', metavar='PATH', action='append',
    help='BAM corresponding to single cells that are to be '
         'analyzed.  May be specified multiple times.')
infiles.add_argument('--bam',  action='append', metavar='PATH',
    help='Additional BAM files that will be included in GATK\'s'
         'joint genotyping, but will otherwise not affect the '
         'analysis.  May be specified several times.')



gatk = config_parser.add_argument_group('Genotyping intervals',
    'These parameters allow the user to specify which portions of the genome should be genotyped.  By default, all autosomal regions will be analyzed.  Regions MUST be specified in the same order as the reference genome and should not overlap!!  The maximum target region is chrs 1-22 and X, due to the SHAPEIT reference panel.  Non-pseudoautosomal regions (PARs) on chrX may also be analyzed, but male samples may fail due to lack of hSNPs.  Initial GATK calling will be performed on each region in parallel if possible, which can greatly decrease runtime.')
gatk.add_argument('--regions', metavar="STRING",
    help='A comma separated list of regions in GATK format: chr:start-stop, e.g. 22:30000001-31000000,22:31000001-32000000.  Cannot be specified in addition to --regions-file.')
gatk.add_argument('--regions-file', metavar='PATH',
    help='A BED file containing regions to be analyzed.  Cannot be specified in addition to --regions.')



caller = config_parser.add_argument_group("Somatic SNV calling parameters. A minimum requirement of 2 reads supporting a putative mutation is probably good practice at most sequencing depths.  However, the minimum total depth for single cell and bulk may need to be altered.  The defaults of 6 and 11, respectively, were successful on single cells with >25x and bulk with >30x mean coverage.")
caller.add_argument('--target-fdr', type=float, metavar='FLOAT',
    help='Desired false discovery rate (FDR).  This is not formal FDR control via, e.g., q-values.  In general, lower values will increase specificity at the cost of sensitivity.')
caller.add_argument('--min-sc-alt', type=int, metavar='INT',
    help='Reject somatic SNVs with fewer than INT reads carrying the mutation in single cells.')
caller.add_argument('--min-sc-dp', type=int, metavar='INT',
    help='Reject somatic SNVs covered by fewer than INT reads in single cells.')
caller.add_argument('--min-bulk-dp', type=int,  metavar='INT',
    help='Reject somatic SNVs covered by fewer than INT reads in bulk.')
caller.add_argument('--no-somatic-snvs', action='store_true',
    help='Do not genotype somatic SNVs.')
caller.add_argument('--somatic-indels', action='store_true',
    help='Enables detection of somatic indels. If this option is specified, a panel of normals must be specified.')
caller.add_argument('--somatic-indel-pon', type=str,
    help='Indel panel of normals to use for indel filtering. Indel calling is NOT precise without a panel of normals.')
caller.add_argument('--mosaic-snvs', action='store_true',
    help='Enable detection of mosaic SNVs.')


phasing = config_parser.add_argument_group('Phasing options',
    'Choose SHAPEIT2 or Eagle for phasing')
phasing.add_argument('--phaser', metavar='eagle|shapeit', type=str,
    help='SHAPEIT2 does not provide a reference panel aligned to hg38. If your data are aligned to this genome version, please use Eagle instead.')
phasing.add_argument('--shapeit-panel', metavar='DIR',
    help='Path to the 1000 genomes project phase 3 SHAPEIT panel.  At this time, other phasing panels are not supported. This panel is required if phaser=shapeit')
phasing.add_argument('--eagle-panel', metavar='DIR',
    help='Path to the an Eagle compatible phasing panel. The reference panel is a set of BCF files, one per chromosome. File names must match the pattern ALL.chr${chromosome}*.bcf. This parameter is required if phaser=eagle.')
phasing.add_argument('--eagle-genmap', metavar='DIR',
    help='Path to Eagle\'s provided genetic map (can be found under the "tables" directory in the Eagle tarball). This is required if phaser=eagle.')


abmodel = config_parser.add_argument_group("AB model fitting",
    "These parameters control the exhaustive parameter search used to fit an AB correlation function for each chromosome.  This is by far the most time consuming step of the calling process.  None of these parameters are used for subsequent AB inference.")
abmodel.add_argument('--abmodel-chunks', type=int, metavar='INT',
    help='Split each AB model sampling stage into INT jobs per chromosome.  When multiple threads or a cluster are available, this will drastically shorten total runtime.')
abmodel.add_argument('--abmodel-samples-per-chunk', type=int, metavar='INT',
    help='Sample the AB model log-likelihood function INT times for each chunk.  The total number of samples for each chromosome will be (--abmodel-chunks) * (--abmodel-samples-per-chunk).')
abmodel.add_argument('--abmodel-hsnp-chunk-size', type=int, metavar='INT',
    help='Approximate the AB model likelihood function for each chromosome by breaking hSNPs into non-overlapping chunks of size INT.  Larger values significantly increase runtime, but may be necessary for organisms with high SNP density (e.g., >1 hSNP per kilobase on average).')
abmodel.add_argument('--abmodel-steps', type=int, metavar='INT',
    help='Refine the parameter space for random sampling INT times.  After the first log-likelihood sampling has completed, a new parameter space is defined so as to contain the 50 points with highest log likelihood.')



spikein = config_parser.add_argument_group("hSNP spike-ins",
    'hSNP spike ins provide a way to estimate sensitivity and the effects of various filters.  hSNP spikeins are also used to train models for excess indel and read clipping, so at least a few thousand spikeins should be used.  However, hSNP spike-ins must be performed in many small batches for two reasons: (1) spike-ins hSNPs are not included in the list of hSNPs used to infer AB and (2) spike-ins are treated as sSNVs in the parameter tuning process which compares sSNV vs. hSNP VAF distributions.  See the SCAN-SNV paper for details.')
spikein.add_argument('--resample-M', type=int, metavar="INT",
    help='Parameter for rejection sampling. Larger M will produce fewer hSNP spikein samples for sensitivity and total mutation burden estimation. However, M must be large enough to enable proper rejection sampling.')



##########################################################################
# The 'validate' subcommand
#
# Tries to ensure the configuration is complete.
validate_parser = subparsers.add_parser('validate',
    help='Validate the configuration parameters of a SCAN2 analysis.'
         '  [ATTENTION: not yet exhaustive.]')
validate_parser.set_defaults(executor=do_validate)


##########################################################################
# The 'run' subcommand
#
# This actually runs an analysis
run_parser = subparsers.add_parser('run',
    help='Run a fully configured SCAN2 analysis.  Always performs a '
         '"validate" subcommand first.')
run_parser.set_defaults(executor=do_run)
run_parser.add_argument('--joblimit', default=1, metavar='INT',
    help='Allow at most INT jobs to execute at any given time.  For '
         'multicore machines or clusters, this can greatly decrease '
         'runtime.')
run_parser.add_argument('--memlimit', default=None, metavar='INT', type=int,
    help="Total available memory in MB.  If unspecified, memory is "
         "treated as unlimited.")
run_parser.add_argument('--scripts', metavar='PATH',
    default='/opt/anaconda1anaconda2anaconda3/lib/scansnv',
    help='Path to SCAN-SNV script files.  Usually points to an installed '
         'set of files.')
run_parser.add_argument('--cluster', default=None, type=str, metavar='ARGS',
    help="Pass ARGS to Snakemake's --cluster parameter.  Do not use "
         "--snakemake-args to access --cluster.  Memory requirements "
         "for each job can be accessed via {resources.mem} and any "
         "instance of '%%logdir' in ARGS will be replaced by "
         "--output-dir/cluster-logs.")
run_parser.add_argument('--drmaa', default=None, type=str, metavar='ARGS',
    help="Pass ARGS to Snakemake's --drmaa parameter.  Do not use "
         "--snakemake-args to access --drmaa.  Memory requirements for "
         "each job can be accessed via {resources.mem} and any instance "
         "of '%%logdir' in ARGS will be replaced by "
         "--output-dir/cluster-logs.")
run_parser.add_argument('--snakemake-args', default='', type=str, metavar='STRING',
    help='STRING is a set of command line arguments to be passed to Snakemake.  Note that a leading space may be necessary, e.g., --snakemake-args " --drmaa \' -p myqueue -t 12:00:00 --mem={resources.mem}\'".')
run_parser.add_argument('--snakefile', metavar='PATH', type=readable_file,
    default='/opt/anaconda1anaconda2anaconda3/lib/scansnv/Snakefile',
    help='Path to the SCAN-SNV Snakefile.  Unlikely to be necessary for standard use.')
run_parser.add_argument('--configfile', metavar='PATH', default=None, type=readable_file,
    help='Path to the Snakemake configuration file.  This file is autogenerated by this script and the user should rarely need to specify their own config file.')


##########################################################################
# The 'show' subcommand
#
# Show all configuration information. Maybe allow for serialization?
# Could just dump the YAML.
show_parser = subparsers.add_parser('show',
    help='Show the configuration of a SCAN2 analysis.')
show_parser.set_defaults(executor=do_show)


##########################################################################
# The 'progress' subcommand
#
# Print some information about the progress of the run:
#   #Jobs complete / Total #Jobs
#   #Jobs with errors
progress_parser = subparsers.add_parser('progress',
    help='Prints statistics regarding the progress of a SCAN2 run.')
progress_parser.set_defaults(executor=do_progress)


# Get the args and run the relavent subcommand
args = ap.parse_args()
if args.subcommand is None:
    print("ERROR: a valid subcommand must be provided. See below.")
    ap.print_help()
    exit(1)

args.analysis_dir = os.path.abspath(args.analysis_dir)
args.executor(args)
